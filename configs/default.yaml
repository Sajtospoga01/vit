compute_precision:
  grad_scaler: True
  teacher:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
  student:
    backbone:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp16
        buffer_dtype: fp32
    dino_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32
    ibot_head:
      sharding_strategy: SHARD_GRAD_OP
      mixed_precision:
        param_dtype: fp16
        reduce_dtype: fp32
        buffer_dtype: fp32

train:
  mini_batch_size: 128
  batch_size: 256
  epochs: 100
  warmup_epochs: 10
  centering: "sinkhorn_knopp"
  output_dir: "/nfs/"

optim:
  name: adamw
  base_lr: 0.0001
  lr: 0.
  final_lr: 1.0e-06
  patch_embed_lr_mult: 0.2
  freeze_last_layer_epochs: 1

  base_wd: 0.04
  final_wd: 0.4
  layerwise_decay: 0.9

  base_momentum: 0.9
  final_momentum: 0.9

  base_teacher_temp: 0.07
  final_teacher_temp: 0.07

  base_last_layer_lr: 0.1
  final_last_layer_lr: 0.1

  clip_grad: 3.0

  adamw_beta1: 0.9
  adamw_beta2: 0.999


# loader parameters

dataloader:
  shuffle: True
  preprocess: True

crops:
  global_crops_scale:
    - 0.32
    - 1.0
  local_crops_scale:
    - 0.05
    - 0.32
  local_crops_number: 8
  local_crops_size: 32
  global_crops_size: 64

# model paramters
dino:
  head_n_prototypes: 65536
  loss_weight: 1.0
  koleo_loss_weight: 0.1
  head_bottleneck_dim: 256
  head_hidden_dim: 2048
  head_nlayers: 3

ibot:
  loss_weight: 1.0
  separate_head: False
  mask_ratio_min_max:
    - 0.1
    - 0.5
  mask_sample_probability: 0.5
  head_nlayers: 3

student:
  arch: "vit_small"
  patch_size: 8
  embed_dim: 768
  depth: 12
  num_heads: 12
  mlp_ratio: 4
  drop_path_rate: 0.3
  layerscale: 1.0e-05
  drop_path_uniform: True
  pretrained_weights: ''
  ffn_layer: "swiglufused"
  block_chunks: 0
  qkv_bias: True
  proj_bias: True
  ffn_bias: True
  num_register_tokens: 4
  interpolate_antialias: False
  interpolate_offset: 0.1

teacher:
  base_m_teacher: 0.992
  final_m_teacher: 1
  warmup_teacher_temp: 0.04
  teacher_temp: 0.07
  warmup_teacher_temp_epochs: 30


io:
  input_size:
    - 64
    - 64
  bands: 32
  output_size:
    - 64
    - 64
  num_classes: NUM_CLASSES
